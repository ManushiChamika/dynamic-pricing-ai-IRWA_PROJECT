=== File: core\agents\data_collector\repo.py ===
from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime, timezone
from typing import Any, Dict, Optional, List

import aiosqlite


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


class DataRepo:
    """
    Minimal repo for market ticks. Uses SQLite at DATA_DB or app/data.db.
    """

    def __init__(self, path: Optional[str] = None) -> None:
        db_env = os.getenv("DATA_DB", "app/data.db")
        self.path = Path(path or db_env)

    async def init(self) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        async with aiosqlite.connect(self.path.as_posix()) as db:
            await db.executescript(
                """
                PRAGMA journal_mode=WAL;

                CREATE TABLE IF NOT EXISTS market_ticks (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  sku TEXT NOT NULL,
                  market TEXT NOT NULL,
                  our_price REAL NOT NULL,
                  competitor_price REAL,
                  demand_index REAL,
                  ts TEXT NOT NULL,
                  source TEXT,
                  ingested_at TEXT NOT NULL
                );

                CREATE INDEX IF NOT EXISTS ix_ticks_sku_market_ts
                  ON market_ticks (sku, market, ts);
                """
            )
            await db.commit()

    async def insert_tick(self, d: Dict[str, Any]) -> None:
        # Expect ISO ts; if missing, use now
        ts = d.get("ts") or _utc_now_iso()
        async with aiosqlite.connect(self.path.as_posix()) as db:
            await db.execute(
                """
                INSERT INTO market_ticks
                  (sku, market, our_price, competitor_price, demand_index, ts,
                   source, ingested_at)
                VALUES (?,?,?,?,?,?,?,?)
                """,
                (
                    d["sku"],
                    d.get("market", "DEFAULT"),
                    float(d["our_price"]),
                    d.get("competitor_price"),
                    d.get("demand_index"),
                    ts,
                    d.get("source", "unknown"),
                    _utc_now_iso(),
                ),
            )
            await db.commit()

    async def features_for(
        self, sku: str, market: str, since_iso: str
    ) -> Dict[str, Any]:
        """
        Return simple recent features for a window: latest values + basic gap.
        """
        q = """
        SELECT our_price, competitor_price, demand_index, ts
        FROM market_ticks
        WHERE sku=? AND market=? AND ts>=?
        ORDER BY ts DESC
        LIMIT 100
        """
        async with aiosqlite.connect(self.path.as_posix()) as db:
            cur = await db.execute(q, (sku, market, since_iso))
            rows = await cur.fetchall()

        if not rows:
            return {
                "snapshot_id": None,
                "as_of": None,
                "features": {},
                "provenance": [],
                "count": 0,
            }

        # Latest row
        our_latest, comp_latest, dem_latest, as_of = rows[0]
        gap_pct = None
        if our_latest and comp_latest is not None:
            try:
                gap_pct = (our_latest - comp_latest) / our_latest if our_latest else None
            except ZeroDivisionError:
                gap_pct = None

        return {
            "snapshot_id": f"snap:{sku}:{market}:{as_of}",
            "as_of": as_of,
            "features": {
                "our_price": our_latest,
                "competitor_price": comp_latest,
                "demand_index": dem_latest,
                "price_gap_pct": gap_pct,
            },
            "provenance": ["market_ticks"],
            "count": len(rows),
        }




=== File: core\agents\market_collector.py ===
# Minimal stub for run_market_collector
import asyncio

async def run_market_collector(sku: str = "SKU-123"):
	print(f"[market_collector] Collecting market data for {sku}...")
	await asyncio.sleep(2)
	print(f"[market_collector] Finished collecting market data for {sku}.")


=== File: core\agents\policy_guard.py ===


=== File: core\agents\price_optimizer\__init__.py ===
"""
Price Optimizer package for Dynamic Pricing project.
Contains:
- optimizer: heuristic optimizer with constraints and rationale.
- mcp_server: MCP tool endpoint for optimize_price (optional if MCP installed).
"""
__all__ = ["optimizer"]




=== File: core\agents\price_optimizer\mcp_server.py ===
from __future__ import annotations

import asyncio
from typing import Dict, Any

try:
    from mcp.server.fastmcp import FastMCP
except Exception as e:
    # Allow file to exist even if MCP isn't installed; runtime will report
    FastMCP = None  # type: ignore

from .optimizer import Features, optimize


async def main():
    if FastMCP is None:
        raise RuntimeError("MCP not available: install the MCP package to run this server.")

    mcp = FastMCP("price-optimizer-service")

    @mcp.tool()
    async def optimize_price(payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        payload: {
          "sku": "...",
          "our_price": 100.0,
          "competitor_price": 98.0,
          "demand_index": 0.5,
          "cost": 90.0,
          "min_price": 80.0,
          "max_price": 130.0,
          "min_margin": 0.12
        }
        """
        f = Features(
            sku=str(payload["sku"]),
            our_price=float(payload["our_price"]),
            competitor_price=payload.get("competitor_price"),
            demand_index=payload.get("demand_index"),
            cost=payload.get("cost"),
        )
        res = optimize(
            f=f,
            min_price=float(payload.get("min_price", 0.0)),
            max_price=float(payload.get("max_price", 1e9)),
            min_margin=float(payload.get("min_margin", 0.12)),
        )
        return res

    await mcp.run()


if __name__ == "__main__":
    asyncio.run(main())




=== File: core\agents\price_optimizer\optimizer.py ===
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Dict, Any


@dataclass
class Features:
    sku: str
    our_price: float
    competitor_price: Optional[float] = None
    demand_index: Optional[float] = None
    cost: Optional[float] = None


def optimize(
    f: Features,
    min_price: float,
    max_price: float,
    min_margin: float = 0.12,
) -> Dict[str, Any]:
    """
    Heuristic v0:
    - Start from our_price.
    - If competitor undercuts by >= ~2%, reduce slightly (to 99% of competitor) within bounds.
    - Enforce margin floor if cost provided.
    - Clamp to [min_price, max_price].
    """
    base = float(f.our_price)
    rationale = []

    # Competitor undercut heuristic
    if f.competitor_price is not None:
        try:
            if f.competitor_price * 1.02 < f.our_price:
                base = max(f.competitor_price * 0.99, min_price)
                rationale.append("Competitor undercut â†’ reduce slightly")
        except Exception:
            pass

    # Enforce margin floor
    if f.cost is not None:
        try:
            floor = f.cost / (1.0 - float(min_margin))
            if base < floor:
                base = floor
                rationale.append("Margin floor enforced")
        except Exception:
            pass

    # Clamp to bounds
    base = min(max(base, min_price), max_price)

    return {
        "recommended_price": round(base, 2),
        "confidence": 0.6,  # placeholder
        "rationale": "; ".join(rationale) or "No change",
        "constraints_evaluation": {
            "min_price": min_price,
            "max_price": max_price,
            "min_margin": min_margin,
        },
    }




=== File: core\agents\pricing_optimizer.py ===
# Minimal stub for run_pricing_optimizer
import asyncio

async def run_pricing_optimizer():
	print("[pricing_optimizer] Running pricing optimizer...")
	await asyncio.sleep(2)
	print("[pricing_optimizer] Finished pricing optimizer run.")
# core/agents/pricing_optimizer.py

import os
import sqlite3
import time
import importlib
from datetime import datetime, timedelta
from pathlib import Path


# Load .env from project root if present (simple loader, no extra deps)
def _load_dotenv_if_present():
	# If key already present, skip
	if os.getenv("OPENAI_API_KEY"):
		return
	# Project root is two parents up from this file (core/agents/...)
	root = Path(__file__).resolve().parents[2]
	env_path = root / ".env"
	if not env_path.exists():
		return
	try:
		with env_path.open("r", encoding="utf-8") as f:
			for line in f:
				line = line.strip()
				if not line or line.startswith("#"):
					continue
				if "=" not in line:
					continue
				k, v = line.split("=", 1)
				k = k.strip()
				v = v.strip().strip('"').strip("'")
				if k and not os.getenv(k):
					os.environ[k] = v
	except Exception:
		pass


# attempt to load .env early
_load_dotenv_if_present()

# --- Step 1: Tools (algorithms) ---

def rule_based(records):
	"""
	Simple rule-based algorithm:
	- Takes competitor prices
	- Returns average competitor price minus 2%
	"""
	prices = [r[0] for r in records]
	if not prices:
		return None
	avg = sum(prices) / len(prices)
	return round(avg * 0.98, 2)


def ml_model(records):
	"""
	Placeholder ML model:
	- Currently just returns average competitor price
	- Later can be replaced with regression/ML
	"""
	prices = [r[0] for r in records]
	if not prices:
		return None
	return round(sum(prices) / len(prices), 2)


def profit_maximization(records):
	"""
	Profit-maximization strategy:
	- Takes competitor prices
	- Returns average competitor price with +10% markup
	"""
	prices = [r[0] for r in records]
	if not prices:
		return None
	avg = sum(prices) / len(prices)
	return round(avg * 1.10, 2)


# Toolbox mapping (dictionary of available tools)
TOOLS = {
	"rule_based": rule_based,
	"ml_model": ml_model,
	"profit_maximization": profit_maximization,
}


# --- Step 2: LLM Brain ---

class LLMBrain:
	"""
	The LLM acts as the 'brain' of the Pricing Optimizer Agent.
	It decides which tool/algorithm to use based on user intent and data context.
	"""

	def __init__(self, api_key: str | None = None, base_url: str | None = None, model: str | None = None):
		# Try to import OpenAI dynamically. If not available, fall back to None
		# Prefer explicitly passed api_key, else try OPENROUTER_API_KEY then OPENAI_API_KEY
		api_key = api_key or os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
		base_url = base_url or os.getenv("OPENROUTER_BASE_URL") or "https://openrouter.ai/api/v1"
		self.model = model or os.getenv("OPENROUTER_MODEL") or os.getenv("OPENAI_MODEL") or "z-ai/glm-4.5-air:free"
		self.client = None
		if api_key:
			try:
				openai_mod = importlib.import_module("openai")
				# client construction depends on library; support passing base_url for OpenRouter
				try:
					# Newer OpenAI SDK accepts base_url and api_key in constructor
					self.client = openai_mod.OpenAI(api_key=api_key, base_url=base_url)
				except Exception:
					# Fallback: set module-level api_key and optionally base_url
					try:
						openai_mod.api_key = api_key
					except Exception:
						pass
					try:
						openai_mod.base_url = base_url
					except Exception:
						pass
					self.client = openai_mod
			except ModuleNotFoundError:
				print("âš ï¸ openai package not installed; LLMBrain will use deterministic fallback")
		else:
			print("âš ï¸ No OpenRouter/OpenAI API key set; LLMBrain will use deterministic fallback")

	def decide_tool(self, user_intent, n_records):
		"""
		Ask the LLM which tool to use.
		"""
		prompt = f"""
		You are the brain of a Pricing Optimizer Agent.
		User intent: {user_intent}
		Number of competitor records: {n_records}

		Available tools:
		- rule_based: use average competitor price - 2%
		- ml_model: use machine learning for large datasets
		- profit_maximization: maximize profit with markup

		Decide the best tool. Answer with only one word:
		rule_based, ml_model, or profit_maximization.
		"""
		try:
			# If no client, use deterministic rule: prefer ml_model for large n, profit if intent contains profit
			if not self.client:
				intent = (user_intent or "").lower()
				if "profit" in intent or "maximize" in intent:
					return "profit_maximization"
				if n_records >= 50:
					return "ml_model"
				return "rule_based"

			resp = self.client.chat.completions.create(
				model=self.model,
				messages=[{"role": "user", "content": prompt}],
				max_tokens=5,
				temperature=0.0,
			)
			choice = resp.choices[0].message.content.strip().lower()
			return choice if choice in TOOLS else "rule_based"
		except Exception as e:
			print("âŒ LLM error:", e)
			return "rule_based"

	def process_full_workflow(self, user_request: str, product_name: str, db_path: str = "market.db", notify_alert_fn=None, wait_seconds: int = 3, max_wait_attempts: int = 5, monitor_timeout: int = 0):
		"""
		Execute the full pricing workflow described in the system prompt.
		Returns strict JSON dict on success or error.

		Parameters:
		- user_request: the user intent/request text
		- product_name: product to price
		- db_path: sqlite DB file
		- notify_alert_fn: optional callable(msg:str) to notify Alert Agent
		- wait_seconds: seconds to wait between simulated Market Data Collector polls
		- max_wait_attempts: how many times to re-check DB after requesting update
		- monitor_timeout: if >0, poll for market_data changes for this many seconds and re-run once
		"""
		# helper: strict error output
		def err(msg: str):
			return {"status": "error", "message": msg}

		# open DB connection locally
		try:
			conn = sqlite3.connect(db_path, check_same_thread=False)
			cursor = conn.cursor()
		except Exception as e:
			return err(f"DB connection failed: {e}")

		# helper: fetch records
		def fetch_records():
			try:
				cursor.execute("SELECT price, update_time FROM market_data WHERE product_name=?", (product_name,))
				return cursor.fetchall()
			except Exception as e:
				return []

		# helper: parse timestamp
		def _parse_time(s):
			if not s:
				return None
			try:
				# Try ISO first
				return datetime.fromisoformat(s)
			except Exception:
				try:
					# fallback common format
					return datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
				except Exception:
					return None

		# Step 1 & 2: Check data freshness and request update if needed
		records = fetch_records()
		stale = False
		if not records:
			stale = True
		else:
			# determine latest update
			latest = None
			for r in records:
				ts = _parse_time(r[1])
				if ts and (latest is None or ts > latest):
					latest = ts
			if not latest or (datetime.now() - latest) > timedelta(hours=24):
				stale = True

		if stale:
			# send market data collect request (simulated)
			msg = f"UPDATE market_data for {product_name}"
			print(msg)
			# Poll DB for confirmation (simulated) up to max_wait_attempts
			attempt = 0
			while attempt < max_wait_attempts:
				time.sleep(wait_seconds)
				records = fetch_records()
				if records:
					# re-evaluate freshness
					latest = None
					for r in records:
						ts = _parse_time(r[1])
						if ts and (latest is None or ts > latest):
							latest = ts
					if latest and (datetime.now() - latest) <= timedelta(hours=24):
						break
				attempt += 1
			if attempt >= max_wait_attempts and (not records or latest is None or (datetime.now() - latest) > timedelta(hours=24)):
				return err("market data not refreshed after request")

		# Step 3: Process data & choose algorithm per rules
		records = fetch_records()
		if not records:
			return err("no market data available")

		n = len(records)
		intent = (user_request or "").lower()
		if "max" in intent and "profit" in intent or "maximize" in intent:
			algo = "profit_maximization"
		elif n < 100:
			algo = "rule_based"
		else:
			algo = "ml_model"

		# Step 4: Calculate price
		try:
			price = TOOLS[algo](records)
		except Exception as e:
			return err(f"calculation failed: {e}")
		if price is None:
			return err("calculation returned no price")

		# Step 5: Update database (insert or update)
		try:
			cursor.execute("SELECT product_name FROM pricing_list WHERE product_name=?", (product_name,))
			exists = cursor.fetchone() is not None
			if exists:
				cursor.execute(
					"UPDATE pricing_list SET optimized_price=?, last_update=CURRENT_TIMESTAMP, reason=? WHERE product_name=?",
					(price, f"optimized using {algo}", product_name),
				)
			else:
				cursor.execute(
					"INSERT INTO pricing_list (product_name, optimized_price, last_update, reason) VALUES (?, ?, CURRENT_TIMESTAMP, ?)",
					(product_name, price, f"optimized using {algo}"),
				)
			conn.commit()
		except Exception as e:
			return err(f"db update failed: {e}")

		# Step 6: Notify Alert Agent
		notify_msg = f"PRICE_UPDATED {product_name} {price}"
		if notify_alert_fn:
			try:
				notify_alert_fn(notify_msg)
			except Exception:
				print("Failed to call notify_alert_fn")
		else:
			print(notify_msg)

		result = {"product": product_name, "price": price, "algorithm": algo, "status": "success"}

		# Step 7: Monitor changes - optional single re-run if market_data updates during monitor_timeout
		if monitor_timeout and monitor_timeout > 0:
			start = time.time()
			# capture current latest update_time
			try:
				cursor.execute("SELECT MAX(update_time) FROM market_data WHERE product_name=?", (product_name,))
				baseline = cursor.fetchone()[0]
			except Exception:
				baseline = None
			while time.time() - start < monitor_timeout:
				time.sleep(1)
				try:
					cursor.execute("SELECT MAX(update_time) FROM market_data WHERE product_name=?", (product_name,))
					latest2 = cursor.fetchone()[0]
				except Exception:
					latest2 = None
				if latest2 and latest2 != baseline:
					# re-run once
					return self.process_full_workflow(user_request, product_name, db_path=db_path, notify_alert_fn=notify_alert_fn, wait_seconds=wait_seconds, max_wait_attempts=max_wait_attempts, monitor_timeout=0)

		return result



# Pricing Optimizer Agent = LLMBrain
# The LLMBrain now IS the Pricing Optimizer Agent. Keep the name for compatibility.
PricingOptimizerAgent = LLMBrain


# --- Example run loop using the unified agent ---
if __name__ == "__main__":
	agent = PricingOptimizerAgent()
	# Use process_full_workflow to run the full workflow end-to-end
	while True:
		res = agent.process_full_workflow("maximize profit", "iphone15")
		print(res)
		time.sleep(30)




=== File: core\agents\pricing_optimizer_bus\bus_events.py ===
# bus_events.py
# Event types for Pricing Optimizer workflow

class PricingOptimizerEvents:
    PROCESSING = "pricing_optimizer_processing"
    CHECKING_DB = "checking_database"
    DATA_STALE = "data_stale"
    UPDATING_DB = "updating_database"
    CALCULATING_PRICE = "calculating_price"
    UPDATING_PRICING_LIST = "updating_pricing_list"
    NOTIFY_ALERT = "notifying_alert_agent"
    DONE = "done"
    ERROR = "error"


=== File: core\agents\pricing_optimizer_bus\bus_iface.py ===
# bus_iface.py
# Interface for bus communication between UI and Pricing Optimizer Agent

class PricingOptimizerBusIface:
    def __init__(self):
        self._listeners = []

    def subscribe(self, callback):
        self._listeners.append(callback)

    def publish(self, message):
        for cb in self._listeners:
            cb(message)

# Singleton bus instance
bus = PricingOptimizerBusIface()


=== File: core\agents\pricing_optimizer_bus\pricing_ui_integration.py ===
# pricing_ui_integration.py
# Simple script to integrate pricing optimizer with UI messaging

import streamlit as st
import threading
import time
import re
from core.agents.pricing_optimizer_bus.bus_iface import bus
from core.agents.pricing_optimizer_bus.bus_events import PricingOptimizerEvents
from core.agents.pricing_optimizer import PricingOptimizerAgent

def handle_pricing_request(user_input):
    """Handle pricing request with real-time status updates"""
    # Extract product name from user input
    product_match = re.search(r'for\s+(\w+)', user_input.lower())
    product_name = product_match.group(1) if product_match else "iphone15"
    
    # Initialize status tracking
    if "pricing_status" not in st.session_state:
        st.session_state["pricing_status"] = "Ready"
    
    # Set up bus listener for status updates
    def status_listener(event_data):
        if isinstance(event_data, dict) and "msg" in event_data:
            st.session_state["pricing_status"] = event_data["msg"]
    
    # Subscribe to bus events
    bus.subscribe(status_listener)
    
    # Run pricing optimizer in background thread
    def run_pricing():
        try:
            agent = PricingOptimizerAgent()
            result = agent.process_full_workflow(user_input, product_name)
            # Store result in session state
            st.session_state["pricing_result"] = result
            if result.get("status") == "success":
                st.session_state["pricing_status"] = f"âœ… Pricing complete for {product_name}: ${result.get('price', 'N/A')} using {result.get('algorithm', 'unknown')} algorithm"
            else:
                st.session_state["pricing_status"] = f"âŒ Error: {result.get('message', 'Unknown error')}"
        except Exception as e:
            st.session_state["pricing_status"] = f"âŒ Error: {str(e)}"
    
    # Start pricing in background
    threading.Thread(target=run_pricing, daemon=True).start()
    
    return f"ðŸ”„ Starting pricing analysis for {product_name}. Please wait..."

def get_pricing_status():
    """Get current pricing status for UI display"""
    return st.session_state.get("pricing_status", "Ready")

def get_pricing_result():
    """Get latest pricing result"""
    return st.session_state.get("pricing_result", None)


=== File: core\agents\user_interact_agent.py ===
# core/agents/user_interact_agent.py
import asyncio
from datetime import datetime
from typing import Dict, Any

from ..bus import bus
from ..protocol import Topic
from ..models import UserRequest, PriceResponse
from .pricing_optimizer import LLMBrain


class UserInteractAgent:
    """Agent that handles user interactions and routes to pricing optimizer."""
    
    def __init__(self, product_name: str = "SKU-123", db_path: str = "market.db"):
        self.product_name = product_name
        self.db_path = db_path
        self.pricing_brain = LLMBrain()
        self.active_requests: Dict[str, UserRequest] = {}
    
    async def start(self):
        """Start the agent and subscribe to user requests."""
        bus.subscribe(Topic.USER_REQUEST.value, self.handle_user_request)
        print(f"UserInteractAgent started, listening for user requests...")
    
    async def handle_user_request(self, request: UserRequest):
        """Handle incoming user request and route to pricing optimizer."""
        try:
            print(f"Processing user request: {request.message} for product: {request.product_name}")
            
            # Store request for tracking
            self.active_requests[request.user_id] = request
            
            # Use the request's product name or fallback to default
            product_name = request.product_name or self.product_name
            
            # Call the pricing optimizer's process_full_workflow method
            def alert_notifier(msg):
                print(f"Alert from pricing optimizer: {msg}")
            
            # Process the request through the pricing optimizer
            result = self.pricing_brain.process_full_workflow(
                user_request=request.message,
                product_name=product_name,
                db_path=self.db_path,
                notify_alert_fn=alert_notifier,
                wait_seconds=1,  # Faster response for chat
                max_wait_attempts=2  # Fewer attempts for real-time feel
            )
            
            # Create response based on result
            if result.get("status") == "success":
                response_message = self.format_success_response(result)
            else:
                response_message = self.format_error_response(result)
            
            # Create and publish response
            response = PriceResponse(
                user_id=request.user_id,
                request_message=request.message,
                response_message=response_message,
                product_name=product_name,
                price=result.get("price"),
                algorithm=result.get("algorithm"),
                status=result.get("status", "error")
            )
            
            # Publish response back to the bus
            await bus.publish(Topic.PRICE_RESPONSE.value, response)
            
            # Clean up completed request
            self.active_requests.pop(request.user_id, None)
            
        except Exception as e:
            # Handle any errors and send error response
            error_response = PriceResponse(
                user_id=request.user_id,
                request_message=request.message,
                response_message=f"Sorry, I encountered an error processing your request: {str(e)}",
                product_name=request.product_name or self.product_name,
                status="error"
            )
            await bus.publish(Topic.PRICE_RESPONSE.value, error_response)
            self.active_requests.pop(request.user_id, None)
    
    def format_success_response(self, result: Dict[str, Any]) -> str:
        """Format a successful pricing result into a user-friendly message."""
        product = result.get("product", "the product")
        price = result.get("price", "N/A")
        algorithm = result.get("algorithm", "standard")
        
        # Create a user-friendly response based on the algorithm used
        if algorithm == "profit_maximization":
            return f"ðŸ’° **Profit Maximization Complete!**\n\nOptimal price for {product}: **${price:.2f}**\n\nThis price is calculated to maximize your profit margins based on current market conditions and demand patterns."
        elif algorithm == "ml_model":
            return f"ðŸ¤– **AI Model Prediction Ready!**\n\nRecommended price for {product}: **${price:.2f}**\n\nThis prediction uses machine learning analysis of market trends, competitor pricing, and historical data patterns."
        elif algorithm == "rule_based":
            return f"ðŸ“Š **Market Analysis Complete!**\n\nSuggested price for {product}: **${price:.2f}**\n\nThis price is based on competitor analysis and rule-based optimization to stay competitive while maintaining margins."
        else:
            return f"âœ… **Pricing Optimization Complete!**\n\nNew price for {product}: **${price:.2f}**\n\nPrice calculated using {algorithm} optimization strategy."
    
    def format_error_response(self, result: Dict[str, Any]) -> str:
        """Format an error result into a user-friendly message."""
        error_msg = result.get("message", "Unknown error occurred")
        
        if "market data" in error_msg.lower():
            return "ðŸ“ˆ **Market Data Update Needed**\n\nI need fresh market data to provide accurate pricing recommendations. The market data collector is working to gather the latest information. Please try again in a moment."
        elif "connection" in error_msg.lower() or "database" in error_msg.lower():
            return "ðŸ”§ **System Maintenance**\n\nI'm experiencing a temporary connection issue. Our technical team is working to resolve this. Please try again shortly."
        else:
            return f"â“ **Processing Issue**\n\nI encountered an issue while processing your request: {error_msg}\n\nPlease try rephrasing your request or contact support if the issue persists."


# Global instance for easy import
user_interact_agent = UserInteractAgent()
print("UserInteractAgent: module imported and global instance created")


=== File: core\auth_db.py ===
from pathlib import Path
from sqlalchemy import (
    create_engine, Column, Integer, String, Boolean, DateTime, func,
    UniqueConstraint, ForeignKey
)
from sqlalchemy.orm import sessionmaker, declarative_base, relationship

# DB at project root (parent of 'core')
BASE_DIR = Path(__file__).resolve().parents[1]
DB_PATH = (BASE_DIR / "auth.db").resolve()

engine = create_engine(f"sqlite:///{DB_PATH}", future=True, echo=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False, expire_on_commit=False)
Base = declarative_base()


class User(Base):
    __tablename__ = "users"
    __table_args__ = (UniqueConstraint("email", name="uq_users_email"),)

    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    full_name = Column(String(255))
    hashed_password = Column(String(512), nullable=False)
    is_active = Column(Boolean, default=True, nullable=False)
    two_factor_enabled = Column(Boolean, default=False, nullable=False)  # âœ… Fixed
    totp_secret = Column(String(64))
    created_at = Column(DateTime, server_default=func.now(), nullable=False)

    sessions = relationship("SessionToken", back_populates="user", cascade="all, delete-orphan")


class SessionToken(Base):
    __tablename__ = "session_tokens"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    token = Column(String(255), unique=True, index=True, nullable=False)
    expires_at = Column(DateTime, nullable=False)
    revoked = Column(Boolean, default=False, nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)

    user = relationship("User", back_populates="sessions")


def init_db():
    """Create tables if they donâ€™t exist."""
    Base.metadata.create_all(bind=engine)
    print(f"Database created at {DB_PATH}")


if __name__ == "__main__":
    init_db()


=== File: core\auth_service.py ===
from typing import Optional
from datetime import datetime, timedelta
import secrets

from argon2 import PasswordHasher
from email_validator import validate_email, EmailNotValidError
from pydantic import BaseModel
from sqlalchemy.exc import IntegrityError

from .auth_db import SessionLocal, User, SessionToken

ph = PasswordHasher()
SESSION_DAYS = 7  # persistent login duration


class RegisterIn(BaseModel):
    email: str
    full_name: Optional[str] = None
    password: str


def _hash(pw: str) -> str:
    return ph.hash(pw)


def _verify(pw: str, h: str) -> bool:
    try:
        ph.verify(h, pw)
        return True
    except Exception:
        return False


# ---------- Register ----------
def register_user(inp: RegisterIn) -> None:
    email_norm = (inp.email or "").strip().lower()
    pw = (inp.password or "").strip()

    # validate email
    try:
        validate_email(email_norm)
    except EmailNotValidError as e:
        raise ValueError(str(e))

    if len(pw) < 10:
        raise ValueError("Password must be at least 10 characters")

    with SessionLocal() as db:
        if db.query(User).filter(User.email == email_norm).first():
            raise ValueError("Email already registered")
        u = User(
            email=email_norm,
            full_name=(inp.full_name or "").strip() or None,
            hashed_password=_hash(pw),
        )
        db.add(u)
        db.commit()


# ---------- Login ----------
def authenticate(email: str, password: str) -> dict:
    email_norm = (email or "").strip().lower()
    pw = (password or "").strip()

    with SessionLocal() as db:
        u = db.query(User).filter(User.email == email_norm).first()
        if not u or not getattr(u, "is_active", True):
            raise ValueError("Invalid email or password")
        try:
            ph.verify(u.hashed_password, pw)
        except Exception:
            raise ValueError("Invalid email or password")

        return {"user_id": u.id, "email": u.email, "full_name": u.full_name}


# ---------- Profile ----------
def get_profile(user_id: int) -> dict:
    with SessionLocal() as db:
        u: Optional[User] = db.get(User, user_id)
        if not u:
            raise ValueError("User not found")
        return {"id": u.id, "email": u.email, "full_name": u.full_name}


# ---------- Persistent session tokens ----------
def create_persistent_session(user_id: int) -> tuple[str, datetime]:
    token = secrets.token_urlsafe(32)
    expires_at = datetime.utcnow() + timedelta(days=SESSION_DAYS)
    with SessionLocal() as db:
        db.add(SessionToken(user_id=user_id, token=token, expires_at=expires_at))
        db.commit()
    return token, expires_at


def validate_session_token(token: str) -> dict | None:
    now = datetime.utcnow()
    with SessionLocal() as db:
        row: Optional[SessionToken] = db.query(SessionToken).filter_by(token=token, revoked=False).first()
        if not row or row.expires_at <= now:
            return None

        u: Optional[User] = db.get(User, row.user_id)
        if not u or not getattr(u, "is_active", True):
            return None

        return {"user_id": u.id, "email": u.email, "full_name": u.full_name}


def revoke_session_token(token: str) -> None:
    with SessionLocal() as db:
        row: Optional[SessionToken] = db.query(SessionToken).filter_by(token=token).first()
        if row:
            row.revoked = True
            db.add(row)
            db.commit()


=== File: core\bus.py ===
# core/bus.py
import asyncio
from typing import Any, Callable, Dict, List
from dataclasses import dataclass


class EventBus:
    """Simple event bus for inter-agent communication."""
    
    def __init__(self):
        self._subscribers: Dict[str, List[Callable]] = {}
        print("EventBus: initialized")
        try:
            from datetime import datetime
            import pathlib
            pathlib.Path.cwd().joinpath("runtime_debug.log").write_text(f"{datetime.utcnow().isoformat()} EventBus initialized\n", encoding="utf-8")
        except Exception:
            pass
    
    def subscribe(self, topic: str, callback: Callable):
        """Subscribe to a topic with a callback function."""
        if topic not in self._subscribers:
            self._subscribers[topic] = []
        self._subscribers[topic].append(callback)
    
    async def publish(self, topic: str, data: Any):
        """Publish data to a topic, calling all subscribers."""
        if topic in self._subscribers:
            for callback in self._subscribers[topic]:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(data)
                    else:
                        callback(data)
                except Exception as e:
                    print(f"Error in subscriber callback for topic {topic}: {e}")
                    try:
                        from datetime import datetime
                        import pathlib
                        pathlib.Path.cwd().joinpath("runtime_debug.log").write_text(f"{datetime.utcnow().isoformat()} Error in subscriber callback for topic {topic}: {e}\n", encoding="utf-8")
                    except Exception:
                        pass
    
    def unsubscribe(self, topic: str, callback: Callable):
        """Unsubscribe a callback from a topic."""
        if topic in self._subscribers:
            self._subscribers[topic] = [cb for cb in self._subscribers[topic] if cb != callback]


# Global bus instance
bus = EventBus()
print("EventBus: global instance created")


=== File: core\create_market_db.py ===
import sqlite3

def create_tables(db_path='market.db'):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Create market_data table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS market_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            product_name TEXT NOT NULL,
            price REAL NOT NULL,
            features TEXT,
            update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # Create pricing_list table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS pricing_list (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            product_name TEXT NOT NULL,
            optimized_price REAL NOT NULL,
            last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            reason TEXT
        )
    ''')

    conn.commit()
    conn.close()

if __name__ == "__main__":
    create_tables()


=== File: core\models.py ===
# core/models.py
from dataclasses import dataclass
from datetime import datetime
from typing import Optional


@dataclass
class UserRequest:
    """User request message for pricing optimization."""
    user_id: str
    message: str
    product_name: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()


@dataclass
class PriceResponse:
    """Response from pricing optimizer agent."""
    user_id: str
    request_message: str
    response_message: str
    product_name: str
    price: Optional[float] = None
    algorithm: Optional[str] = None
    status: str = "success"
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()


@dataclass
class MarketTick:
    """Market data tick event."""
    sku: str
    our_price: float
    competitor_price: Optional[float] = None
    demand_index: float = 0.0
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()


@dataclass
class PriceProposal:
    """Price proposal from pricing agent."""
    sku: str
    proposed_price: float
    current_price: float
    margin: float
    algorithm: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()


@dataclass
class AlertEvent:
    """Alert event from alert notifier."""
    sku: str
    kind: str
    message: str
    severity: str  # "info", "warn", "crit"
    ts: datetime = None
    
    def __post_init__(self):
        if self.ts is None:
            self.ts = datetime.utcnow()


=== File: core\protocol.py ===
# core/protocol.py
from enum import Enum


class Topic(Enum):
    """Event bus topics for inter-agent communication."""
    
    # User interaction topics
    USER_REQUEST = "user_request"
    PRICE_RESPONSE = "price_response"
    
    # Market data topics
    MARKET_TICK = "market_tick"
    MARKET_UPDATE = "market_update"
    
    # Pricing topics
    PRICE_PROPOSAL = "price_proposal"
    PRICE_UPDATE = "price_update"
    
    # Alert topics
    ALERT = "alert"
    
    # Agent lifecycle topics
    AGENT_START = "agent_start"
    AGENT_STOP = "agent_stop"


=== File: docs\price_optimizer.md ===


=== File: export_code_files.ps1 ===
param(
    [string]$Name = "default"
)

# Get current git branch
$branch = git rev-parse --abbrev-ref HEAD 2>$null
if (-not $branch) { $branch = "no-branch" }

# Set output file name
$outputFile = "all_code_files_${branch}_$Name.txt"

# Remove the output file if it already exists
if (Test-Path $outputFile) { Remove-Item $outputFile }

# Define file extensions to include
$includeExtensions = @("*.py", "*.md", "*.txt")

# Get all code files recursively, excluding unnecessary files and folders
$codeFiles = Get-ChildItem -Path . -Recurse -Include $includeExtensions -File |
    Where-Object {
        $_.FullName -notmatch "__pycache__|\.venv|\.git|env|Lib|site-packages|\.pyc$|\.pyo$|\.db$"
    }

foreach ($file in $codeFiles) {
    Add-Content -Path $outputFile -Value "===== $($file.FullName) ====="
    Get-Content $file.FullName | Add-Content -Path $outputFile
    Add-Content -Path $outputFile -Value "`n"
}

Write-Host "All code files have been combined into $outputFile"


=== File: files.txt ===
:\Users\SASINDU\Desktop\IRWA Group Repo\dynamic-pricing-ai-IRWA_PROJECT\README.md
:\Users\SASINDU\Desktop\IRWA Group Repo\dynamic-pricing-ai-IRWA_PROJECT\requirements.txt


=== File: project_bundle_manager.py ===
#!/usr/bin/env python3
"""
Project Bundle Manager - Collect and Split Project Code

Collects all relevant project files and splits them into parts under a token limit.

Usage:
    python project_bundle_manager.py --token-limit 30000

Dependencies:
    - tiktoken (optional, recommended): pip install tiktoken
"""

import argparse
import math
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple


def now():
    """Get current timestamp for logging."""
    return datetime.now().isoformat(sep=' ', timespec='seconds')


def log(msg: str, log_path: str):
    """Write a timestamped message to the log file."""
    line = f"[{now()}] {msg}"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line + '\n')


class Tokenizer:
    """Token counter with tiktoken support and fallback."""
    
    def __init__(self, model='cl100k_base'):
        self.backend = 'fallback'
        try:
            import tiktoken
            try:
                enc = tiktoken.encoding_for_model(model)
            except Exception:
                try:
                    enc = tiktoken.get_encoding(model)
                except Exception:
                    enc = tiktoken.get_encoding('cl100k_base')
            self.encoder = enc
            self.backend = 'tiktoken'
        except Exception:
            self.encoder = None

    def count(self, text: str) -> int:
        """Count tokens in text."""
        if self.backend == 'tiktoken' and self.encoder is not None:
            try:
                tokens = self.encoder.encode(text)
                return len(tokens)
            except Exception:
                return math.ceil(len(text) / 4)
        else:
            return math.ceil(len(text) / 4)


class ProjectBundleManager:
    """Manages collection and splitting of project code bundles."""
    
    # File extensions to include
    TEXT_EXTENSIONS = {
        '.py', '.md', '.txt', '.json', '.yaml', '.yml', '.ini', '.cfg',
        '.js', '.ts', '.html', '.css', '.java', '.c', '.cpp', '.h',
        '.rb', '.go', '.rs', '.sh', '.ps1', '.sql'
    }
    
    # Directories to exclude
    EXCLUDE_DIRS = {
        '.git', '__pycache__', 'build', 'dist', 'node_modules', 'venv',
        '.venv', 'env', 'site-packages', '.egg-info', '.parcel-cache', 'vendor'
    }
    
    # Special files to always include
    SPECIAL_FILES = {'requirements.txt', 'README.md'}
    
    def __init__(self, log_file: str = 'project_bundle_manager.log'):
        self.log_file = log_file
        self.included = 0
        self.skipped = 0
        self.errors = 0
        self.skipped_large = 0
        self.skipped_binary = 0
        self.skipped_extension = 0
        self.excluded_dirs = {}  # dir_name -> file_count
        self.ignored_extensions = {}  # extension -> count
    
    def is_text_file(self, file_path: Path) -> bool:
        """Check if a file is a text file by looking for null bytes."""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\x00' not in chunk
        except Exception:
            return False
    
    def should_exclude_path(self, path: Path) -> bool:
        """Check if a path should be excluded based on directory names."""
        for part in path.parts:
            if part in self.EXCLUDE_DIRS:
                return True
        return False
    
    def should_include_file(self, file_path: Path) -> Tuple[bool, str]:
        """
        Determine if a file should be included in the bundle.
        Returns (should_include, reason).
        """
        # Check size (max 5MB)
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > 5:
                self.skipped_large += 1
                return False, f"large ({size_mb:.1f}MB > 5MB)"
        except Exception:
            return False, "cannot read file stats"
        
        # Check if it's in an excluded directory
        if self.should_exclude_path(file_path):
            # Track which excluded directory this file is in
            for part in file_path.parts:
                if part in self.EXCLUDE_DIRS:
                    self.excluded_dirs[part] = self.excluded_dirs.get(part, 0) + 1
                    break
            return False, "in excluded directory"
        
        # Check if it's a special file
        if file_path.name in self.SPECIAL_FILES:
            if self.is_text_file(file_path):
                return True, "special file"
            else:
                self.skipped_binary += 1
                return False, "special file but binary"
        
        # Check extension
        if file_path.suffix.lower() in self.TEXT_EXTENSIONS:
            if self.is_text_file(file_path):
                return True, "text file"
            else:
                self.skipped_binary += 1
                return False, "has text extension but is binary"
        
        self.skipped_extension += 1
        # Track ignored file extensions
        ext = file_path.suffix.lower() if file_path.suffix else '(no extension)'
        self.ignored_extensions[ext] = self.ignored_extensions.get(ext, 0) + 1
        return False, "not a recognized text file"
    
    def collect_and_split(self, token_limit: int = 30000, output_prefix: str = 'project_code_bundle_part'):
        """Collect all files and split them into parts under token limit."""
        root_dir = Path.cwd()
        tokenizer = Tokenizer()
        
        # Create timestamped output folder
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = f"bundle_{timestamp}_limit{token_limit}"
        output_folder = root_dir / folder_name
        output_folder.mkdir(exist_ok=True)
        
        # Update log file path to be inside the output folder
        self.log_file = output_folder / "bundle_creation.log"
        
        print(f"Tokenizer backend: {tokenizer.backend}")
        print(f"Collecting files from: {root_dir}")
        print(f"Output folder: {output_folder}")
        
        # Clear log file
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write("")
        log("Starting project bundle collection and split", self.log_file)
        
        # Find all files
        try:
            all_files = list(root_dir.rglob('*'))
            all_files = [f for f in all_files if f.is_file()]
        except Exception as e:
            print(f"Error scanning directory: {e}")
            return
        
        # Filter files
        included_files = []
        large_files = []
        
        for file_path in all_files:
            should_include, reason = self.should_include_file(file_path)
            
            if should_include:
                included_files.append(file_path)
            else:
                self.skipped += 1
                # Track large files specifically for detailed reporting
                if "large" in reason:
                    rel_path = file_path.relative_to(root_dir)
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    large_files.append((rel_path, size_mb))
        
        # Print detailed statistics
        print(f"\n=== SCAN RESULTS ===")
        print(f"âœ… Files to include: {len(included_files)}")
        print(f"âŒ Files skipped: {self.skipped}")
        
        if self.excluded_dirs:
            print(f"\nðŸ“ Excluded directories and file counts:")
            for dir_name, count in sorted(self.excluded_dirs.items()):
                print(f"   {dir_name}: {count} files")
        
        if large_files:
            print(f"\nðŸ“ Large files (>5MB) ignored:")
            for file_path, size_mb in large_files:
                print(f"   {file_path} ({size_mb:.1f}MB)")
        
        if self.skipped_binary > 0:
            print(f"\nðŸ”’ Binary files ignored: {self.skipped_binary}")
        
        if self.skipped_extension > 0:
            print(f"\nðŸ“„ Files with unrecognized extensions ignored: {self.skipped_extension}")
            if self.ignored_extensions:
                print("   Extensions found:")
                for ext, count in sorted(self.ignored_extensions.items(), key=lambda x: x[1], reverse=True):
                    print(f"     {ext}: {count} files")
        
        print(f"\n" + "="*50)
        
        if not included_files:
            print("No files to process.")
            return
        
        # Process files and split into parts
        current_part = 1
        current_blocks = []
        current_tokens = 0
        created = 0
        
        for file_path in sorted(included_files):
            try:
                rel_path = file_path.relative_to(root_dir)
                
                # Read file content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Create file block
                header = f"=== File: {rel_path} ==="
                block = f"{header}\n{content}"
                block_tokens = tokenizer.count(block)
                
                # Check if we need to start a new part
                if current_blocks and (current_tokens + block_tokens) > token_limit:
                    # Write current part
                    out_name = output_folder / f"{output_prefix}{current_part}.txt"
                    self._write_part(out_name, current_blocks)
                    log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
                    created += 1
                    
                    current_part += 1
                    current_blocks = []
                    current_tokens = 0
                
                # Add block to current part
                current_blocks.append(block)
                current_tokens += block_tokens
                self.included += 1
                
                log(f"Added {rel_path} to part {current_part} (tokens: {block_tokens})", self.log_file)
                
            except Exception as e:
                self.errors += 1
                log(f"ERROR processing {rel_path}: {e}", self.log_file)
        
        # Write remaining blocks
        if current_blocks:
            out_name = output_folder / f"{output_prefix}{current_part}.txt"
            self._write_part(out_name, current_blocks)
            log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
            created += 1
        
        print(f"\nDone! Created {created} part(s) in folder: {folder_name}")
        print(f"Included: {self.included}, Skipped: {self.skipped}, Errors: {self.errors}")
        print(f"See {self.log_file} for details.")
        
        # Create summary file
        self._create_summary_file(output_folder, created, token_limit, timestamp)
    
    def _write_part(self, output_path, blocks: List[str]):
        """Write blocks to a part file with proper line endings."""
        text = '\n\n'.join(blocks)
        text = text.replace('\n', '\r\n')
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            f.write(text)
    
    def _create_summary_file(self, output_folder, parts_created, token_limit, timestamp):
        """Create a summary file with bundle creation details."""
        summary_file = output_folder / "BUNDLE_SUMMARY.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("PROJECT BUNDLE SUMMARY\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Creation Time: {timestamp}\n")
            f.write(f"Token Limit: {token_limit:,}\n")
            f.write(f"Parts Created: {parts_created}\n")
            f.write(f"Files Included: {self.included}\n")
            f.write(f"Files Skipped: {self.skipped}\n")
            f.write(f"Errors: {self.errors}\n\n")
            
            if self.excluded_dirs:
                f.write("EXCLUDED DIRECTORIES:\n")
                f.write("-" * 30 + "\n")
                for dir_name, count in sorted(self.excluded_dirs.items()):
                    f.write(f"  {dir_name}: {count} files\n")
                f.write("\n")
            
            if self.skipped_large > 0:
                f.write(f"LARGE FILES IGNORED: {self.skipped_large}\n")
                f.write("(Files larger than 5MB)\n\n")
            
            if self.skipped_binary > 0:
                f.write(f"BINARY FILES IGNORED: {self.skipped_binary}\n\n")
            
            if self.ignored_extensions:
                f.write("IGNORED FILE EXTENSIONS:\n")
                f.write("-" * 30 + "\n")
                for ext, count in sorted(self.ignored_extensions.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"  {ext}: {count} files\n")
                f.write("\n")
            
            f.write("FILES INCLUDED:\n")
            f.write("-" * 30 + "\n")
            for i in range(1, parts_created + 1):
                f.write(f"  project_code_bundle_part{i}.txt\n")
            
            f.write(f"\nLog file: bundle_creation.log\n")
        
        print(f"ðŸ“„ Summary saved to: {summary_file}")
        print(f"ðŸ“ All files are in: {output_folder}")


def main():
    parser = argparse.ArgumentParser(description='Project Bundle Manager - Collect and Split Project Code')
    parser.add_argument('--token-limit', '-t', type=int, default=30000, 
                       help='Token limit per part (default: 30000)')
    parser.add_argument('--out-prefix', default='project_code_bundle_part', 
                       help='Output prefix for split parts (default: project_code_bundle_part)')
    parser.add_argument('--log-file', default='project_bundle_manager.log', 
                       help='Log file (default: project_bundle_manager.log)')
    
    args = parser.parse_args()
    
    manager = ProjectBundleManager(args.log_file)
    
    try:
        manager.collect_and_split(args.token_limit, args.out_prefix)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()


=== File: project_bundle_manager_simple.py ===
#!/usr/bin/env python3
"""
Project Bundle Manager - Collect and Split Project Code

Collects all relevant project files and splits them into parts under a token limit.

Usage:
    python project_bundle_manager.py --token-limit 30000

Dependencies:
    - tiktoken (optional, recommended): pip install tiktoken
"""

import argparse
import math
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple


def now():
    """Get current timestamp for logging."""
    return datetime.now().isoformat(sep=' ', timespec='seconds')


def log(msg: str, log_path: str):
    """Write a timestamped message to the log file."""
    line = f"[{now()}] {msg}"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line + '\n')


class Tokenizer:
    """Token counter with tiktoken support and fallback."""
    
    def __init__(self, model='cl100k_base'):
        self.backend = 'fallback'
        try:
            import tiktoken
            try:
                enc = tiktoken.encoding_for_model(model)
            except Exception:
                try:
                    enc = tiktoken.get_encoding(model)
                except Exception:
                    enc = tiktoken.get_encoding('cl100k_base')
            self.encoder = enc
            self.backend = 'tiktoken'
        except Exception:
            self.encoder = None

    def count(self, text: str) -> int:
        """Count tokens in text."""
        if self.backend == 'tiktoken' and self.encoder is not None:
            try:
                tokens = self.encoder.encode(text)
                return len(tokens)
            except Exception:
                return math.ceil(len(text) / 4)
        else:
            return math.ceil(len(text) / 4)


class ProjectBundleManager:
    """Manages collection and splitting of project code bundles."""
    
    # File extensions to include
    TEXT_EXTENSIONS = {
        '.py', '.md', '.txt', '.json', '.yaml', '.yml', '.ini', '.cfg',
        '.js', '.ts', '.html', '.css', '.java', '.c', '.cpp', '.h',
        '.rb', '.go', '.rs', '.sh', '.ps1', '.sql'
    }
    
    # Directories to exclude
    EXCLUDE_DIRS = {
        '.git', '__pycache__', 'build', 'dist', 'node_modules', 'venv',
        '.venv', 'env', 'site-packages', '.egg-info', '.parcel-cache', 'vendor'
    }
    
    # Special files to always include
    SPECIAL_FILES = {'requirements.txt', 'README.md'}
    
    def __init__(self, log_file: str = 'project_bundle_manager.log'):
        self.log_file = log_file
        self.included = 0
        self.skipped = 0
        self.errors = 0
    
    def is_text_file(self, file_path: Path) -> bool:
        """Check if a file is a text file by looking for null bytes."""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\x00' not in chunk
        except Exception:
            return False
    
    def should_exclude_path(self, path: Path) -> bool:
        """Check if a path should be excluded based on directory names."""
        for part in path.parts:
            if part in self.EXCLUDE_DIRS:
                return True
        return False
    
    def should_include_file(self, file_path: Path) -> Tuple[bool, str]:
        """
        Determine if a file should be included in the bundle.
        Returns (should_include, reason).
        """
        # Check size (max 5MB)
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > 5:
                return False, f"large (>5MB)"
        except Exception:
            return False, "cannot read file stats"
        
        # Check if it's in an excluded directory
        if self.should_exclude_path(file_path):
            return False, "in excluded directory"
        
        # Check if it's a special file
        if file_path.name in self.SPECIAL_FILES:
            if self.is_text_file(file_path):
                return True, "special file"
            else:
                return False, "special file but binary"
        
        # Check extension
        if file_path.suffix.lower() in self.TEXT_EXTENSIONS:
            if self.is_text_file(file_path):
                return True, "text file"
            else:
                return False, "has text extension but is binary"
        
        return False, "not a recognized text file"
    
    def collect_and_split(self, token_limit: int = 30000, output_prefix: str = 'project_code_bundle_part'):
        """Collect all files and split them into parts under token limit."""
        root_dir = Path.cwd()
        tokenizer = Tokenizer()
        
        print(f"Tokenizer backend: {tokenizer.backend}")
        print(f"Collecting files from: {root_dir}")
        
        # Clear log file
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write("")
        log("Starting project bundle collection and split", self.log_file)
        
        # Find all files
        try:
            all_files = list(root_dir.rglob('*'))
            all_files = [f for f in all_files if f.is_file()]
        except Exception as e:
            print(f"Error scanning directory: {e}")
            return
        
        # Filter files
        included_files = []
        for file_path in all_files:
            should_include, reason = self.should_include_file(file_path)
            
            if should_include:
                included_files.append(file_path)
            else:
                self.skipped += 1
        
        print(f"Found {len(included_files)} files to include, skipped {self.skipped}")
        
        if not included_files:
            print("No files to process.")
            return
        
        # Process files and split into parts
        current_part = 1
        current_blocks = []
        current_tokens = 0
        created = 0
        
        for file_path in sorted(included_files):
            try:
                rel_path = file_path.relative_to(root_dir)
                
                # Read file content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Create file block
                header = f"=== File: {rel_path} ==="
                block = f"{header}\n{content}"
                block_tokens = tokenizer.count(block)
                
                # Check if we need to start a new part
                if current_blocks and (current_tokens + block_tokens) > token_limit:
                    # Write current part
                    out_name = f"{output_prefix}{current_part}.txt"
                    self._write_part(out_name, current_blocks)
                    log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
                    created += 1
                    
                    current_part += 1
                    current_blocks = []
                    current_tokens = 0
                
                # Add block to current part
                current_blocks.append(block)
                current_tokens += block_tokens
                self.included += 1
                
                log(f"Added {rel_path} to part {current_part} (tokens: {block_tokens})", self.log_file)
                
            except Exception as e:
                self.errors += 1
                log(f"ERROR processing {rel_path}: {e}", self.log_file)
        
        # Write remaining blocks
        if current_blocks:
            out_name = f"{output_prefix}{current_part}.txt"
            self._write_part(out_name, current_blocks)
            log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
            created += 1
        
        print(f"\nDone! Created {created} part(s).")
        print(f"Included: {self.included}, Skipped: {self.skipped}, Errors: {self.errors}")
        print(f"See {self.log_file} for details.")
    
    def _write_part(self, output_path: str, blocks: List[str]):
        """Write blocks to a part file with proper line endings."""
        text = '\n\n'.join(blocks)
        text = text.replace('\n', '\r\n')
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            f.write(text)


def main():
    parser = argparse.ArgumentParser(description='Project Bundle Manager - Collect and Split Project Code')
    parser.add_argument('--token-limit', '-t', type=int, default=30000, 
                       help='Token limit per part (default: 30000)')
    parser.add_argument('--out-prefix', default='project_code_bundle_part', 
                       help='Output prefix for split parts (default: project_code_bundle_part)')
    parser.add_argument('--log-file', default='project_bundle_manager.log', 
                       help='Log file (default: project_bundle_manager.log)')
    
    args = parser.parse_args()
    
    manager = ProjectBundleManager(args.log_file)
    
    try:
        manager.collect_and_split(args.token_limit, args.out_prefix)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()


=== File: README.md ===
# dynamic-pricing-ai-IRWA_PROJECT  

=== File: requirements.txt ===
streamlit==1.38.0
sqlalchemy==2.0.36
argon2-cffi==23.1.0
pyotp==2.9.0
email-validator==2.2.0
pydantic==2.8.2
extra-streamlit-components==0.1.71
qrcode==7.4.2
Pillow==10.4.0


=== File: scripts\insert_mock_market_data.py ===
import sqlite3
import random
from datetime import datetime

DB = 'market.db'
PRODUCT = 'iphone15'
COUNT = 85

conn = sqlite3.connect(DB)
cur = conn.cursor()
cur.execute("CREATE TABLE IF NOT EXISTS market_data (id INTEGER PRIMARY KEY AUTOINCREMENT, product_name TEXT NOT NULL, price REAL NOT NULL, update_time TEXT DEFAULT CURRENT_TIMESTAMP)")
cur.execute("CREATE TABLE IF NOT EXISTS pricing_list (id INTEGER PRIMARY KEY AUTOINCREMENT, product_name TEXT NOT NULL, optimized_price REAL NOT NULL, last_update TEXT DEFAULT CURRENT_TIMESTAMP, reason TEXT)")
cur.execute('DELETE FROM market_data WHERE product_name=?', (PRODUCT,))
for _ in range(COUNT):
    price = round(1100 + random.uniform(-50, 200), 2)
    cur.execute('INSERT INTO market_data (product_name, price, update_time) VALUES (?,?,?)', (PRODUCT, price, datetime.now().isoformat()))
conn.commit()
print(f'inserted {COUNT} records for {PRODUCT} into {DB}')
conn.close()


=== File: scripts\run_pricing_agent_once.py ===
#!/usr/bin/env python3
"""Run the Pricing Optimizer once and write results to app/ for inspection.

This script is intended to be executed with the project's venv Python.
"""
import json
import sqlite3
import os
import traceback

from core.agents.pricing_optimizer import PricingOptimizerAgent


def main():
    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    out_dir = os.path.join(repo_root, "app")
    os.makedirs(out_dir, exist_ok=True)
    result_path = os.path.join(out_dir, "pricing_agent_run.json")
    dbrows_path = os.path.join(out_dir, "pricing_agent_dbrows.json")

    try:
        agent = PricingOptimizerAgent()
        res = agent.process_full_workflow("maximize profit", "iphone15")
    except Exception as e:
        res = {"status": "error", "message": str(e), "trace": traceback.format_exc()}

    # write agent result
    try:
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump({"agent_result": res}, f, default=str, indent=2)
    except Exception:
        print("failed to write agent result:\n", traceback.format_exc())

    # query DB for pricing_list row
    try:
        conn = sqlite3.connect(os.path.join(repo_root, "market.db"))
        cur = conn.cursor()
        cur.execute("SELECT product_name, optimized_price, last_update, reason FROM pricing_list WHERE product_name=?", ("iphone15",))
        rows = cur.fetchall()
        conn.close()
    except Exception as e:
        rows = {"status": "db_error", "message": str(e), "trace": traceback.format_exc()}

    try:
        with open(dbrows_path, "w", encoding="utf-8") as f:
            json.dump({"db_rows": rows}, f, default=str, indent=2)
    except Exception:
        print("failed to write db rows:\n", traceback.format_exc())

    print("WROTE", result_path, dbrows_path)


if __name__ == '__main__':
    main()


=== File: scripts\smoke_data_collector.py ===
from __future__ import annotations

import asyncio
from datetime import datetime, timezone

from core.agents.data_collector.repo import DataRepo
from core.agents.data_collector.collector import DataCollector
from core.agents.data_collector.connectors.mock import mock_ticks


async def main():
    repo = DataRepo()  # uses DATA_DB or app/data.db
    await repo.init()
    dc = DataCollector(repo)

    # Ingest a few mock ticks
    await dc.ingest_stream(mock_ticks(n=3), delay_s=0.1)

    # Fetch features for the last day
    since_iso = (datetime.now(timezone.utc)).isoformat()
    # We inserted just now, so features_for with wide window:
    res = await repo.features_for("SKU-123", "DEFAULT", "1970-01-01T00:00:00+00:00")
    print("FEATURES:", res)


if __name__ == "__main__":
    asyncio.run(main())




=== File: scripts\smoke_price_optimizer.py ===
