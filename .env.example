# Example environment variables for FluxPricer AI
# Copy this file to `.env` and fill in the values locally. Do NOT commit real keys.

# Gemini API configuration
# Sign up at https://ai.google.dev and create an API key
GEMINI_API_KEY=
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# Gemini models: gemini-1.5-pro for summarization, gemini-2.0-flash-exp for chat (default)
GEMINI_PRO_MODEL=gemini-1.5-pro
GEMINI_FLASH_MODEL=gemini-2.0-flash-exp
# Optional: add more Gemini keys for fallback
# GEMINI_API_KEY_2=
# GEMINI_API_KEY_3=

# Optional fallback providers
# OpenRouter (free GLM 4.5)
# OPENROUTER_API_KEY=
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# OPENROUTER_MODEL=z-ai/glm-4.5-air:free

# OpenAI
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4o-mini

# Debug logging for LLM client (0/1)
DEBUG_LLM=1

# Optional: enable UI sound notifications (0/1)
SOUND_NOTIFICATIONS=1

# UI behavior
# Require login to access the app (0/1)
UI_REQUIRE_LOGIN=0

# MCP integration toggle and settings
# When set to 1/true/on, Supervisor uses MCP stdio to call
# data collector server; otherwise it calls local Python directly.
USE_MCP=0
# Override MCP server command/args if needed (defaults shown)
# MCP_DC_CMD=python
# MCP_DC_ARGS=-u -m core.agents.data_collector.mcp_server

# -------------------------------
# Developer mode + UI controls
# -------------------------------
# Developer mode turns on timestamps, metadata panel, and thinking tokens by default in the UI.
# 0 (off) | 1 (on)
DEV_MODE=0

# Cap the number of tokens requested by the UI chat calls to the LLM.
# If 0 or unset, defaults to 1024 in code.
UI_LLM_MAX_TOKENS=1024

# LLM price map for cost estimation (per 1K tokens). Optional.
# Provide as a single-line JSON object. Keys can be either "provider:model" or just model name.
# Example:
# LLM_PRICE_MAP={"openai:gpt-4o-mini":{"in":0.005,"out":0.015},"openrouter:deepseek/deepseek-r1-0528:free":{"in":0,"out":0}}
LLM_PRICE_MAP=

# -------------------------------
# Memory assembly (summary + tail)
# -------------------------------
# Maximum number of recent messages to include when there is no summary yet.
UI_HISTORY_MAX_MSGS=24
# Number of tail messages to include after the latest summary.
UI_HISTORY_TAIL_AFTER_SUMMARY=12

# -------------------------------
# Rolling summarization triggers
# -------------------------------
# Summarize after at least this many messages since last summary.
UI_SUMMARIZE_AFTER_MSGS=12
# Consider thread "long" beyond this many total messages; enable probabilistic summarization.
UI_LONG_THREAD_MSGS=20
# Trigger summarization if combined prompt+completion tokens exceed this threshold for a turn.
UI_SUMMARIZE_TOKEN_TRIGGER=2000
# Probability to summarize on long threads when other triggers didn't fire (0.0 - 1.0).
UI_SUMMARIZE_PROB=0.25

# -------------------------------
# Summarizer model (uses Gemini Pro for better quality summaries)
SUMMARIZER_MODEL=gemini-2.5-pro
