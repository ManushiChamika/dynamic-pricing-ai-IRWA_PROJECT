# Example environment variables for FluxPricer AI
# Copy this file to `.env` and fill in the values locally. Do NOT commit real keys.

# Preferred provider: OpenRouter
# Sign up at https://openrouter.ai and create an API key.
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# Choose a model available to your account; defaults below are inexpensive/free tiers when available
OPENROUTER_MODEL=z-ai/glm-4.5-air:free
# Optional headers for OpenRouter analytics (not required)
# OPENROUTER_REFERRER=http://localhost:8501
# OPENROUTER_TITLE=FluxPricer AI

# Optional: Use OpenAI instead (only if you have access)
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4o-mini

# Debug logging for LLM client (0/1)
DEBUG_LLM=1

# Optional: enable UI sound notifications (0/1)
SOUND_NOTIFICATIONS=1

# UI behavior
# Require login to access the app (0/1)
UI_REQUIRE_LOGIN=0

# MCP integration toggle and settings
# When set to 1/true/on, Supervisor uses MCP stdio to call
# data collector server; otherwise it calls local Python directly.
USE_MCP=0
# Override MCP server command/args if needed (defaults shown)
# MCP_DC_CMD=python
# MCP_DC_ARGS=-u -m core.agents.data_collector.mcp_server

# -------------------------------
# Developer mode + UI controls
# -------------------------------
# Developer mode turns on timestamps, metadata panel, and thinking tokens by default in the UI.
# 0 (off) | 1 (on)
DEV_MODE=0

# Cap the number of tokens requested by the UI chat calls to the LLM.
# If 0 or unset, defaults to 1024 in code.
UI_LLM_MAX_TOKENS=1024

# LLM price map for cost estimation (per 1K tokens). Optional.
# Provide as a single-line JSON object. Keys can be either "provider:model" or just model name.
# Example:
# LLM_PRICE_MAP={"openai:gpt-4o-mini":{"in":0.005,"out":0.015},"openrouter:deepseek/deepseek-r1-0528:free":{"in":0,"out":0}}
LLM_PRICE_MAP=

# -------------------------------
# Memory assembly (summary + tail)
# -------------------------------
# Maximum number of recent messages to include when there is no summary yet.
UI_HISTORY_MAX_MSGS=24
# Number of tail messages to include after the latest summary.
UI_HISTORY_TAIL_AFTER_SUMMARY=12

# -------------------------------
# Rolling summarization triggers
# -------------------------------
# Summarize after at least this many messages since last summary.
UI_SUMMARIZE_AFTER_MSGS=12
# Consider thread "long" beyond this many total messages; enable probabilistic summarization.
UI_LONG_THREAD_MSGS=20
# Trigger summarization if combined prompt+completion tokens exceed this threshold for a turn.
UI_SUMMARIZE_TOKEN_TRIGGER=2000
# Probability to summarize on long threads when other triggers didn't fire (0.0 - 1.0).
UI_SUMMARIZE_PROB=0.25

# -------------------------------
# Summarizer model override (optional)
# -------------------------------
# If set, backend summary generation uses this model name directly.
# Leave unset to use the default provider/model selection logic.
SUMMARIZER_MODEL=
