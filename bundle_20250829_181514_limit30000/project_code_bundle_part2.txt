=== File: core\auth_service.py ===
from typing import Optional
from datetime import datetime, timedelta
import secrets

from argon2 import PasswordHasher
from email_validator import validate_email, EmailNotValidError
from pydantic import BaseModel
from sqlalchemy.exc import IntegrityError

from .auth_db import SessionLocal, User, SessionToken

ph = PasswordHasher()
SESSION_DAYS = 7  # persistent login duration


class RegisterIn(BaseModel):
    email: str
    full_name: Optional[str] = None
    password: str


def _hash(pw: str) -> str:
    return ph.hash(pw)


def _verify(pw: str, h: str) -> bool:
    try:
        ph.verify(h, pw)
        return True
    except Exception:
        return False


# ---------- Register ----------
def register_user(inp: RegisterIn) -> None:
    email_norm = (inp.email or "").strip().lower()
    pw = (inp.password or "").strip()

    # validate email
    try:
        validate_email(email_norm)
    except EmailNotValidError as e:
        raise ValueError(str(e))

    if len(pw) < 10:
        raise ValueError("Password must be at least 10 characters")

    with SessionLocal() as db:
        if db.query(User).filter(User.email == email_norm).first():
            raise ValueError("Email already registered")
        u = User(
            email=email_norm,
            full_name=(inp.full_name or "").strip() or None,
            hashed_password=_hash(pw),
        )
        db.add(u)
        db.commit()


# ---------- Login ----------
def authenticate(email: str, password: str) -> dict:
    email_norm = (email or "").strip().lower()
    pw = (password or "").strip()

    with SessionLocal() as db:
        u = db.query(User).filter(User.email == email_norm).first()
        if not u or not getattr(u, "is_active", True):
            raise ValueError("Invalid email or password")
        try:
            ph.verify(u.hashed_password, pw)
        except Exception:
            raise ValueError("Invalid email or password")

        return {"user_id": u.id, "email": u.email, "full_name": u.full_name}


# ---------- Profile ----------
def get_profile(user_id: int) -> dict:
    with SessionLocal() as db:
        u: Optional[User] = db.get(User, user_id)
        if not u:
            raise ValueError("User not found")
        return {"id": u.id, "email": u.email, "full_name": u.full_name}


# ---------- Persistent session tokens ----------
def create_persistent_session(user_id: int) -> tuple[str, datetime]:
    token = secrets.token_urlsafe(32)
    expires_at = datetime.utcnow() + timedelta(days=SESSION_DAYS)
    with SessionLocal() as db:
        db.add(SessionToken(user_id=user_id, token=token, expires_at=expires_at))
        db.commit()
    return token, expires_at


def validate_session_token(token: str) -> dict | None:
    now = datetime.utcnow()
    with SessionLocal() as db:
        row: Optional[SessionToken] = db.query(SessionToken).filter_by(token=token, revoked=False).first()
        if not row or row.expires_at <= now:
            return None

        u: Optional[User] = db.get(User, row.user_id)
        if not u or not getattr(u, "is_active", True):
            return None

        return {"user_id": u.id, "email": u.email, "full_name": u.full_name}


def revoke_session_token(token: str) -> None:
    with SessionLocal() as db:
        row: Optional[SessionToken] = db.query(SessionToken).filter_by(token=token).first()
        if row:
            row.revoked = True
            db.add(row)
            db.commit()


=== File: core\brokers\types.py ===
# core/brokers/types.py
import json
from datetime import datetime, timezone
from typing import Any

def to_jsonable(obj: Any) -> Any:
    """Make any pydantic/dataclass/obj JSON-serializable with ISO datetimes."""
    if hasattr(obj, "model_dump"):
        obj = obj.model_dump()
    elif hasattr(obj, "dict"):
        obj = obj.dict()
    elif hasattr(obj, "__dict__"):
        obj = obj.__dict__
    def _conv(o):
        if isinstance(o, datetime):
            return o.astimezone(timezone.utc).isoformat()
        raise TypeError(f"Not JSON serializable: {type(o)}")
    return json.loads(json.dumps(obj, default=_conv))


=== File: core\graphs\alert_authoring_graph.py ===
from langgraph.graph import StateGraph, END
from typing import Dict, Any
from pydantic import BaseModel, ValidationError
from core.agents.alert_service.schemas import RuleSpec
from core.agents.alert_service.tools import Tools
from core.agents.alert_service.repo import Repo

class GState(BaseModel):
    user_text: str
    draft_rule: Dict[str, Any] | None = None
    validated: bool = False
    result: Dict[str, Any] | None = None

tools = Tools(Repo())

def nl_to_draft(state: GState) -> GState:
    # very small deterministic parser; you can swap in an LLM with structured output
    txt = state.user_text.lower()
    spec = {
        "id": "auto-undercut",
        "source": "MARKET_TICK",
        "where": "tick.competitor_price and tick.competitor_price * 1.02 < tick.our_price",
        "hold_for": "5m",
        "severity": "warn",
        "notify": {"channels": ["ui"], "throttle": "15m"},
        "enabled": True
    }
    state.draft_rule = spec
    return state

def validate_rule(state: GState) -> GState:
    try:
        RuleSpec(**state.draft_rule)
        state.validated = True
    except ValidationError as e:
        state.result = {"ok": False, "error": e.errors()}
    return state

def persist_rule(state: GState) -> GState:
    if not state.validated: return state
    state.result = {"ok": True}
    # call tool directly (synchronous here; wrap async in your app)
    import asyncio
    asyncio.get_event_loop().run_until_complete(tools.create_rule(state.draft_rule))
    return state

graph = StateGraph(GState)
graph.add_node("parse", nl_to_draft)
graph.add_node("validate", validate_rule)
graph.add_node("persist", persist_rule)
graph.add_edge("parse","validate")
graph.add_edge("validate","persist")
graph.set_entry_point("parse")
graph.set_finish_point("persist")
app = graph.compile()


=== File: core\graphs\incident_triage_graph.py ===
from langgraph.graph import StateGraph
from pydantic import BaseModel
from core.agents.alert_service.tools import Tools
from core.agents.alert_service.repo import Repo

class TriageState(BaseModel):
    command: str
    incident_id: str | None = None
    action: str | None = None
    result: dict | None = None

tools = Tools(Repo())

def parse_cmd(s: TriageState) -> TriageState:
    t = s.command.lower()
    if t.startswith("ack "): s.action, s.incident_id = "ACK", t.split()[1]
    elif t.startswith("resolve "): s.action, s.incident_id = "RESOLVE", t.split()[1]
    else: s.action = "LIST"
    return s

def act(s: TriageState) -> TriageState:
    import asyncio
    if s.action == "ACK": s.result = asyncio.get_event_loop().run_until_complete(tools.ack_incident(s.incident_id))
    elif s.action == "RESOLVE": s.result = asyncio.get_event_loop().run_until_complete(tools.resolve_incident(s.incident_id))
    else: s.result = asyncio.get_event_loop().run_until_complete(tools.list_incidents("OPEN"))
    return s

graph = StateGraph(TriageState)
graph.add_node("parse", parse_cmd)
graph.add_node("act", act)
graph.add_edge("parse","act")
graph.set_entry_point("parse")
app = graph.compile()


=== File: docs\data_collector.md ===
# Data Collector â€” Developer Notes

Purpose:
- Ingest market ticks (our_price, competitor_price, demand_index).
- Persist ticks to app/data.db (aiosqlite).
- Expose MCP tools (fetch_market_features, ingest_tick) and publish MARKET_TICK events.

Key files:
- core/agents/data_collector/repo.py
- core/agents/data_collector/collector.py
- core/agents/data_collector/connectors/mock.py
- scripts/ingest_demo.py

DB:
- Use app/data.db for market ticks (do NOT commit DB file).
- Add DATA_DB path to local .env in each worktree.

I/O contract:
- MarketTick: { sku, our_price, competitor_price?, demand_index, ts, source }
- fetch_market_features(sku, market, time_window) => { snapshot_id, as_of, features, provenance }


=== File: docs\price_optimizer.md ===


=== File: files.txt ===
:\Users\SASINDU\Desktop\IRWA Group Repo\dynamic-pricing-ai-IRWA_PROJECT\README.md
:\Users\SASINDU\Desktop\IRWA Group Repo\dynamic-pricing-ai-IRWA_PROJECT\requirements.txt


=== File: project_bundle_manager.py ===
#!/usr/bin/env python3
"""
Project Bundle Manager - Collect and Split Project Code

Collects all relevant project files and splits them into parts under a token limit.

Usage:
    python project_bundle_manager.py --token-limit 30000

Dependencies:
    - tiktoken (optional, recommended): pip install tiktoken
"""

import argparse
import math
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple


def now():
    """Get current timestamp for logging."""
    return datetime.now().isoformat(sep=' ', timespec='seconds')


def log(msg: str, log_path: str):
    """Write a timestamped message to the log file."""
    line = f"[{now()}] {msg}"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line + '\n')


class Tokenizer:
    """Token counter with tiktoken support and fallback."""
    
    def __init__(self, model='cl100k_base'):
        self.backend = 'fallback'
        try:
            import tiktoken
            try:
                enc = tiktoken.encoding_for_model(model)
            except Exception:
                try:
                    enc = tiktoken.get_encoding(model)
                except Exception:
                    enc = tiktoken.get_encoding('cl100k_base')
            self.encoder = enc
            self.backend = 'tiktoken'
        except Exception:
            self.encoder = None

    def count(self, text: str) -> int:
        """Count tokens in text."""
        if self.backend == 'tiktoken' and self.encoder is not None:
            try:
                tokens = self.encoder.encode(text)
                return len(tokens)
            except Exception:
                return math.ceil(len(text) / 4)
        else:
            return math.ceil(len(text) / 4)


class ProjectBundleManager:
    """Manages collection and splitting of project code bundles."""
    
    # File extensions to include
    TEXT_EXTENSIONS = {
        '.py', '.md', '.txt', '.json', '.yaml', '.yml', '.ini', '.cfg',
        '.js', '.ts', '.html', '.css', '.java', '.c', '.cpp', '.h',
        '.rb', '.go', '.rs', '.sh', '.ps1', '.sql'
    }
    
    # Directories to exclude
    EXCLUDE_DIRS = {
        '.git', '__pycache__', 'build', 'dist', 'node_modules', 'venv',
        '.venv', 'env', 'site-packages', '.egg-info', '.parcel-cache', 'vendor'
    }
    
    # Special files to always include
    SPECIAL_FILES = {'requirements.txt', 'README.md'}
    
    def __init__(self, log_file: str = 'project_bundle_manager.log'):
        self.log_file = log_file
        self.included = 0
        self.skipped = 0
        self.errors = 0
        self.skipped_large = 0
        self.skipped_binary = 0
        self.skipped_extension = 0
        self.excluded_dirs = {}  # dir_name -> file_count
        self.ignored_extensions = {}  # extension -> count
    
    def is_text_file(self, file_path: Path) -> bool:
        """Check if a file is a text file by looking for null bytes."""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\x00' not in chunk
        except Exception:
            return False
    
    def should_exclude_path(self, path: Path) -> bool:
        """Check if a path should be excluded based on directory names."""
        for part in path.parts:
            if part in self.EXCLUDE_DIRS:
                return True
        return False
    
    def should_include_file(self, file_path: Path) -> Tuple[bool, str]:
        """
        Determine if a file should be included in the bundle.
        Returns (should_include, reason).
        """
        # Check size (max 5MB)
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > 5:
                self.skipped_large += 1
                return False, f"large ({size_mb:.1f}MB > 5MB)"
        except Exception:
            return False, "cannot read file stats"
        
        # Check if it's in an excluded directory
        if self.should_exclude_path(file_path):
            # Track which excluded directory this file is in
            for part in file_path.parts:
                if part in self.EXCLUDE_DIRS:
                    self.excluded_dirs[part] = self.excluded_dirs.get(part, 0) + 1
                    break
            return False, "in excluded directory"
        
        # Check if it's a special file
        if file_path.name in self.SPECIAL_FILES:
            if self.is_text_file(file_path):
                return True, "special file"
            else:
                self.skipped_binary += 1
                return False, "special file but binary"
        
        # Check extension
        if file_path.suffix.lower() in self.TEXT_EXTENSIONS:
            if self.is_text_file(file_path):
                return True, "text file"
            else:
                self.skipped_binary += 1
                return False, "has text extension but is binary"
        
        self.skipped_extension += 1
        # Track ignored file extensions
        ext = file_path.suffix.lower() if file_path.suffix else '(no extension)'
        self.ignored_extensions[ext] = self.ignored_extensions.get(ext, 0) + 1
        return False, "not a recognized text file"
    
    def collect_and_split(self, token_limit: int = 30000, output_prefix: str = 'project_code_bundle_part'):
        """Collect all files and split them into parts under token limit."""
        root_dir = Path.cwd()
        tokenizer = Tokenizer()
        
        # Create timestamped output folder
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = f"bundle_{timestamp}_limit{token_limit}"
        output_folder = root_dir / folder_name
        output_folder.mkdir(exist_ok=True)
        
        # Update log file path to be inside the output folder
        self.log_file = output_folder / "bundle_creation.log"
        
        print(f"Tokenizer backend: {tokenizer.backend}")
        print(f"Collecting files from: {root_dir}")
        print(f"Output folder: {output_folder}")
        
        # Clear log file
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write("")
        log("Starting project bundle collection and split", self.log_file)
        
        # Find all files
        try:
            all_files = list(root_dir.rglob('*'))
            all_files = [f for f in all_files if f.is_file()]
        except Exception as e:
            print(f"Error scanning directory: {e}")
            return
        
        # Filter files
        included_files = []
        large_files = []
        
        for file_path in all_files:
            should_include, reason = self.should_include_file(file_path)
            
            if should_include:
                included_files.append(file_path)
            else:
                self.skipped += 1
                # Track large files specifically for detailed reporting
                if "large" in reason:
                    rel_path = file_path.relative_to(root_dir)
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    large_files.append((rel_path, size_mb))
        
        # Print detailed statistics
        print(f"\n=== SCAN RESULTS ===")
        print(f"âœ… Files to include: {len(included_files)}")
        print(f"âŒ Files skipped: {self.skipped}")
        
        if self.excluded_dirs:
            print(f"\nðŸ“ Excluded directories and file counts:")
            for dir_name, count in sorted(self.excluded_dirs.items()):
                print(f"   {dir_name}: {count} files")
        
        if large_files:
            print(f"\nðŸ“ Large files (>5MB) ignored:")
            for file_path, size_mb in large_files:
                print(f"   {file_path} ({size_mb:.1f}MB)")
        
        if self.skipped_binary > 0:
            print(f"\nðŸ”’ Binary files ignored: {self.skipped_binary}")
        
        if self.skipped_extension > 0:
            print(f"\nðŸ“„ Files with unrecognized extensions ignored: {self.skipped_extension}")
            if self.ignored_extensions:
                print("   Extensions found:")
                for ext, count in sorted(self.ignored_extensions.items(), key=lambda x: x[1], reverse=True):
                    print(f"     {ext}: {count} files")
        
        print(f"\n" + "="*50)
        
        if not included_files:
            print("No files to process.")
            return
        
        # Process files and split into parts
        current_part = 1
        current_blocks = []
        current_tokens = 0
        created = 0
        
        for file_path in sorted(included_files):
            try:
                rel_path = file_path.relative_to(root_dir)
                
                # Read file content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Create file block
                header = f"=== File: {rel_path} ==="
                block = f"{header}\n{content}"
                block_tokens = tokenizer.count(block)
                
                # Check if we need to start a new part
                if current_blocks and (current_tokens + block_tokens) > token_limit:
                    # Write current part
                    out_name = output_folder / f"{output_prefix}{current_part}.txt"
                    self._write_part(out_name, current_blocks)
                    log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
                    created += 1
                    
                    current_part += 1
                    current_blocks = []
                    current_tokens = 0
                
                # Add block to current part
                current_blocks.append(block)
                current_tokens += block_tokens
                self.included += 1
                
                log(f"Added {rel_path} to part {current_part} (tokens: {block_tokens})", self.log_file)
                
            except Exception as e:
                self.errors += 1
                log(f"ERROR processing {rel_path}: {e}", self.log_file)
        
        # Write remaining blocks
        if current_blocks:
            out_name = output_folder / f"{output_prefix}{current_part}.txt"
            self._write_part(out_name, current_blocks)
            log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
            created += 1
        
        print(f"\nDone! Created {created} part(s) in folder: {folder_name}")
        print(f"Included: {self.included}, Skipped: {self.skipped}, Errors: {self.errors}")
        print(f"See {self.log_file} for details.")
        
        # Create summary file
        self._create_summary_file(output_folder, created, token_limit, timestamp)
    
    def _write_part(self, output_path, blocks: List[str]):
        """Write blocks to a part file with proper line endings."""
        text = '\n\n'.join(blocks)
        text = text.replace('\n', '\r\n')
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            f.write(text)
    
    def _create_summary_file(self, output_folder, parts_created, token_limit, timestamp):
        """Create a summary file with bundle creation details."""
        summary_file = output_folder / "BUNDLE_SUMMARY.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("PROJECT BUNDLE SUMMARY\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Creation Time: {timestamp}\n")
            f.write(f"Token Limit: {token_limit:,}\n")
            f.write(f"Parts Created: {parts_created}\n")
            f.write(f"Files Included: {self.included}\n")
            f.write(f"Files Skipped: {self.skipped}\n")
            f.write(f"Errors: {self.errors}\n\n")
            
            if self.excluded_dirs:
                f.write("EXCLUDED DIRECTORIES:\n")
                f.write("-" * 30 + "\n")
                for dir_name, count in sorted(self.excluded_dirs.items()):
                    f.write(f"  {dir_name}: {count} files\n")
                f.write("\n")
            
            if self.skipped_large > 0:
                f.write(f"LARGE FILES IGNORED: {self.skipped_large}\n")
                f.write("(Files larger than 5MB)\n\n")
            
            if self.skipped_binary > 0:
                f.write(f"BINARY FILES IGNORED: {self.skipped_binary}\n\n")
            
            if self.ignored_extensions:
                f.write("IGNORED FILE EXTENSIONS:\n")
                f.write("-" * 30 + "\n")
                for ext, count in sorted(self.ignored_extensions.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"  {ext}: {count} files\n")
                f.write("\n")
            
            f.write("FILES INCLUDED:\n")
            f.write("-" * 30 + "\n")
            for i in range(1, parts_created + 1):
                f.write(f"  project_code_bundle_part{i}.txt\n")
            
            f.write(f"\nLog file: bundle_creation.log\n")
        
        print(f"ðŸ“„ Summary saved to: {summary_file}")
        print(f"ðŸ“ All files are in: {output_folder}")


def main():
    parser = argparse.ArgumentParser(description='Project Bundle Manager - Collect and Split Project Code')
    parser.add_argument('--token-limit', '-t', type=int, default=30000, 
                       help='Token limit per part (default: 30000)')
    parser.add_argument('--out-prefix', default='project_code_bundle_part', 
                       help='Output prefix for split parts (default: project_code_bundle_part)')
    parser.add_argument('--log-file', default='project_bundle_manager.log', 
                       help='Log file (default: project_bundle_manager.log)')
    
    args = parser.parse_args()
    
    manager = ProjectBundleManager(args.log_file)
    
    try:
        manager.collect_and_split(args.token_limit, args.out_prefix)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()


=== File: project_bundle_manager_simple.py ===
#!/usr/bin/env python3
"""
Project Bundle Manager - Collect and Split Project Code

Collects all relevant project files and splits them into parts under a token limit.

Usage:
    python project_bundle_manager.py --token-limit 30000

Dependencies:
    - tiktoken (optional, recommended): pip install tiktoken
"""

import argparse
import math
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple


def now():
    """Get current timestamp for logging."""
    return datetime.now().isoformat(sep=' ', timespec='seconds')


def log(msg: str, log_path: str):
    """Write a timestamped message to the log file."""
    line = f"[{now()}] {msg}"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line + '\n')


class Tokenizer:
    """Token counter with tiktoken support and fallback."""
    
    def __init__(self, model='cl100k_base'):
        self.backend = 'fallback'
        try:
            import tiktoken
            try:
                enc = tiktoken.encoding_for_model(model)
            except Exception:
                try:
                    enc = tiktoken.get_encoding(model)
                except Exception:
                    enc = tiktoken.get_encoding('cl100k_base')
            self.encoder = enc
            self.backend = 'tiktoken'
        except Exception:
            self.encoder = None

    def count(self, text: str) -> int:
        """Count tokens in text."""
        if self.backend == 'tiktoken' and self.encoder is not None:
            try:
                tokens = self.encoder.encode(text)
                return len(tokens)
            except Exception:
                return math.ceil(len(text) / 4)
        else:
            return math.ceil(len(text) / 4)


class ProjectBundleManager:
    """Manages collection and splitting of project code bundles."""
    
    # File extensions to include
    TEXT_EXTENSIONS = {
        '.py', '.md', '.txt', '.json', '.yaml', '.yml', '.ini', '.cfg',
        '.js', '.ts', '.html', '.css', '.java', '.c', '.cpp', '.h',
        '.rb', '.go', '.rs', '.sh', '.ps1', '.sql'
    }
    
    # Directories to exclude
    EXCLUDE_DIRS = {
        '.git', '__pycache__', 'build', 'dist', 'node_modules', 'venv',
        '.venv', 'env', 'site-packages', '.egg-info', '.parcel-cache', 'vendor'
    }
    
    # Special files to always include
    SPECIAL_FILES = {'requirements.txt', 'README.md'}
    
    def __init__(self, log_file: str = 'project_bundle_manager.log'):
        self.log_file = log_file
        self.included = 0
        self.skipped = 0
        self.errors = 0
    
    def is_text_file(self, file_path: Path) -> bool:
        """Check if a file is a text file by looking for null bytes."""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\x00' not in chunk
        except Exception:
            return False
    
    def should_exclude_path(self, path: Path) -> bool:
        """Check if a path should be excluded based on directory names."""
        for part in path.parts:
            if part in self.EXCLUDE_DIRS:
                return True
        return False
    
    def should_include_file(self, file_path: Path) -> Tuple[bool, str]:
        """
        Determine if a file should be included in the bundle.
        Returns (should_include, reason).
        """
        # Check size (max 5MB)
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > 5:
                return False, f"large (>5MB)"
        except Exception:
            return False, "cannot read file stats"
        
        # Check if it's in an excluded directory
        if self.should_exclude_path(file_path):
            return False, "in excluded directory"
        
        # Check if it's a special file
        if file_path.name in self.SPECIAL_FILES:
            if self.is_text_file(file_path):
                return True, "special file"
            else:
                return False, "special file but binary"
        
        # Check extension
        if file_path.suffix.lower() in self.TEXT_EXTENSIONS:
            if self.is_text_file(file_path):
                return True, "text file"
            else:
                return False, "has text extension but is binary"
        
        return False, "not a recognized text file"
    
    def collect_and_split(self, token_limit: int = 30000, output_prefix: str = 'project_code_bundle_part'):
        """Collect all files and split them into parts under token limit."""
        root_dir = Path.cwd()
        tokenizer = Tokenizer()
        
        print(f"Tokenizer backend: {tokenizer.backend}")
        print(f"Collecting files from: {root_dir}")
        
        # Clear log file
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write("")
        log("Starting project bundle collection and split", self.log_file)
        
        # Find all files
        try:
            all_files = list(root_dir.rglob('*'))
            all_files = [f for f in all_files if f.is_file()]
        except Exception as e:
            print(f"Error scanning directory: {e}")
            return
        
        # Filter files
        included_files = []
        for file_path in all_files:
            should_include, reason = self.should_include_file(file_path)
            
            if should_include:
                included_files.append(file_path)
            else:
                self.skipped += 1
        
        print(f"Found {len(included_files)} files to include, skipped {self.skipped}")
        
        if not included_files:
            print("No files to process.")
            return
        
        # Process files and split into parts
        current_part = 1
        current_blocks = []
        current_tokens = 0
        created = 0
        
        for file_path in sorted(included_files):
            try:
                rel_path = file_path.relative_to(root_dir)
                
                # Read file content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Create file block
                header = f"=== File: {rel_path} ==="
                block = f"{header}\n{content}"
                block_tokens = tokenizer.count(block)
                
                # Check if we need to start a new part
                if current_blocks and (current_tokens + block_tokens) > token_limit:
                    # Write current part
                    out_name = f"{output_prefix}{current_part}.txt"
                    self._write_part(out_name, current_blocks)
                    log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
                    created += 1
                    
                    current_part += 1
                    current_blocks = []
                    current_tokens = 0
                
                # Add block to current part
                current_blocks.append(block)
                current_tokens += block_tokens
                self.included += 1
                
                log(f"Added {rel_path} to part {current_part} (tokens: {block_tokens})", self.log_file)
                
            except Exception as e:
                self.errors += 1
                log(f"ERROR processing {rel_path}: {e}", self.log_file)
        
        # Write remaining blocks
        if current_blocks:
            out_name = f"{output_prefix}{current_part}.txt"
            self._write_part(out_name, current_blocks)
            log(f"Wrote {out_name} (blocks={len(current_blocks)}, tokens~{current_tokens})", self.log_file)
            created += 1
        
        print(f"\nDone! Created {created} part(s).")
        print(f"Included: {self.included}, Skipped: {self.skipped}, Errors: {self.errors}")
        print(f"See {self.log_file} for details.")
    
    def _write_part(self, output_path: str, blocks: List[str]):
        """Write blocks to a part file with proper line endings."""
        text = '\n\n'.join(blocks)
        text = text.replace('\n', '\r\n')
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            f.write(text)


def main():
    parser = argparse.ArgumentParser(description='Project Bundle Manager - Collect and Split Project Code')
    parser.add_argument('--token-limit', '-t', type=int, default=30000, 
                       help='Token limit per part (default: 30000)')
    parser.add_argument('--out-prefix', default='project_code_bundle_part', 
                       help='Output prefix for split parts (default: project_code_bundle_part)')
    parser.add_argument('--log-file', default='project_bundle_manager.log', 
                       help='Log file (default: project_bundle_manager.log)')
    
    args = parser.parse_args()
    
    manager = ProjectBundleManager(args.log_file)
    
    try:
        manager.collect_and_split(args.token_limit, args.out_prefix)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()


=== File: README.md ===
# dynamic-pricing-ai-IRWA_PROJECT
.env
app/alert.db


=== File: requirements.txt ===
streamlit==1.38.0
sqlalchemy==2.0.36
argon2-cffi==23.1.0
pyotp==2.9.0
email-validator==2.2.0
pydantic==2.8.2
extra-streamlit-components==0.1.71
qrcode==7.4.2
Pillow==10.4.0
plotly
aiosqlite
aiohttp
aiosmtplib
python-dotenv
pandas
langgraph
mcp
transformers
# torch may require a platform-specific wheel; install separately if fails
torch
nats-py


=== File: scripts\ingest_demo.py ===
# scripts/ingest_demo.py
"""
Run a small demo ingestion using the mock connector.
Usage: python scripts/ingest_demo.py
"""
import asyncio
import os, sys
HERE = os.path.dirname(__file__)
ROOT = os.path.abspath(os.path.join(HERE, ".."))
sys.path.insert(0, ROOT)

from core.agents.data_collector.repo import DataRepo
from core.agents.data_collector.collector import DataCollector
from core.agents.data_collector.connectors.mock import mock_ticks

async def main():
    repo = DataRepo(path="app/data.db")
    await repo.init()
    coll = DataCollector(repo)
    for tick in mock_ticks(n=8):
        await coll.ingest_tick(tick)
    print("Ingest demo completed.")

if __name__ == "__main__":
    asyncio.run(main())


=== File: scripts\smoke_data_collector.py ===
from __future__ import annotations

import asyncio
from datetime import datetime, timezone

from core.agents.data_collector.repo import DataRepo
from core.agents.data_collector.collector import DataCollector
from core.agents.data_collector.connectors.mock import mock_ticks


async def main():
    repo = DataRepo()  # uses DATA_DB or app/data.db
    await repo.init()
    dc = DataCollector(repo)

    # Ingest a few mock ticks
    await dc.ingest_stream(mock_ticks(n=3), delay_s=0.1)

    # Fetch features for the last day
    since_iso = (datetime.now(timezone.utc)).isoformat()
    # We inserted just now, so features_for with wide window:
    res = await repo.features_for("SKU-123", "DEFAULT", "1970-01-01T00:00:00+00:00")
    print("FEATURES:", res)


if __name__ == "__main__":
    asyncio.run(main())




=== File: scripts\smoke_price_optimizer.py ===
